{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @version : 05/2023\n",
    "# @author : grafika_jati\n",
    "# LiDAR cover contaminant classification using classic ML for 1D LiDAR aggregated data\n",
    "\n",
    "# Split train and test from dataset taken by # B. Schlager, T. Goelles, S. Muckenhuber and D. Watzenig, \"Contaminations on Lidar Sensor Covers: Performance Degradation Including Fault Detection and Modeling as Potential Applications,\" in IEEE Open Journal of Intelligent Transportation Systems, vol. 3, pp. 738-747, 2022, doi: 10.1109/OJITS.2022.3214094.\n",
    "\n",
    "# Train : exp 1,2,3,4\n",
    "# Test : exp 5\n",
    "\n",
    "#import the libabries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import time as ti\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "import pickle\n",
    "from sklearn.metrics import classification_report # performance measurement , untuk menghasilkan confusion matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot confusion matrix\n",
    "def plot_confusion_matrix(model_name, cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig('1D_class_norm_per_exp_train_test_conf/1D_class_norm_per_exp_train_test-'+model_name+'.png')\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all dataset\n",
    "df = pd.read_csv(\"../get_1D_dataset_Norm_per_exp_train_test/dataset_1D_distribution_XYZ_RA_7_feature_exp.csv\")\n",
    "\n",
    "df.drop(df[df['class'] == 'dew'].index, inplace = True)\n",
    "df.drop(df[df['class'] == 'dirt_05mm'].index, inplace = True)\n",
    "df.drop(df[df['class'] == 'dirt_10mm'].index, inplace = True)\n",
    "df.drop(df[df['class'] == 'dirt_15mm'].index, inplace = True)\n",
    "\n",
    "df_add = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1346453/478114593.py:52: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_A_train = df_A_train.append(df)\n",
      "/tmp/ipykernel_1346453/478114593.py:52: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_A_train = df_A_train.append(df)\n",
      "/tmp/ipykernel_1346453/478114593.py:52: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_A_train = df_A_train.append(df)\n",
      "/tmp/ipykernel_1346453/478114593.py:52: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_A_train = df_A_train.append(df)\n",
      "/tmp/ipykernel_1346453/478114593.py:52: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_A_train = df_A_train.append(df)\n",
      "/tmp/ipykernel_1346453/478114593.py:52: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_A_train = df_A_train.append(df)\n",
      "/tmp/ipykernel_1346453/478114593.py:52: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_A_train = df_A_train.append(df)\n",
      "/tmp/ipykernel_1346453/478114593.py:52: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_A_train = df_A_train.append(df)\n",
      "/tmp/ipykernel_1346453/478114593.py:52: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_A_train = df_A_train.append(df)\n",
      "/tmp/ipykernel_1346453/478114593.py:52: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_A_train = df_A_train.append(df)\n",
      "/tmp/ipykernel_1346453/478114593.py:52: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_A_train = df_A_train.append(df)\n",
      "/tmp/ipykernel_1346453/478114593.py:52: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_A_train = df_A_train.append(df)\n",
      "/tmp/ipykernel_1346453/478114593.py:52: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_A_train = df_A_train.append(df)\n",
      "/tmp/ipykernel_1346453/478114593.py:52: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_A_train = df_A_train.append(df)\n",
      "/tmp/ipykernel_1346453/478114593.py:52: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_A_train = df_A_train.append(df)\n",
      "/tmp/ipykernel_1346453/478114593.py:52: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_A_train = df_A_train.append(df)\n",
      "/tmp/ipykernel_1346453/478114593.py:52: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_A_train = df_A_train.append(df)\n",
      "/tmp/ipykernel_1346453/478114593.py:52: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_A_train = df_A_train.append(df)\n",
      "/tmp/ipykernel_1346453/478114593.py:59: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_A_test = df_A_test.append(df)\n",
      "/tmp/ipykernel_1346453/478114593.py:59: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_A_test = df_A_test.append(df)\n",
      "/tmp/ipykernel_1346453/478114593.py:59: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_A_test = df_A_test.append(df)\n",
      "/tmp/ipykernel_1346453/478114593.py:59: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_A_test = df_A_test.append(df)\n",
      "/tmp/ipykernel_1346453/478114593.py:59: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_A_test = df_A_test.append(df)\n",
      "/tmp/ipykernel_1346453/478114593.py:59: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_A_test = df_A_test.append(df)\n"
     ]
    }
   ],
   "source": [
    "#get data class=ref without cover\n",
    "# 80% for traning, 20% for testing\n",
    "df_ref =df_add[df_add['class'] == 'clean']\n",
    "df_ref_train = df_ref.sample(frac=0.80,random_state=1)\n",
    "df_ref_test = df_ref.drop(df_ref_train.index)\n",
    "\n",
    "#get data class=ref with cover\n",
    "# 80% for traning, 20% for testing\n",
    "df_ref_wh =df_add[df_add['class'] == 'cover']\n",
    "df_ref_wh_train = df_ref_wh.sample(frac=0.80,random_state=1)\n",
    "df_ref_wh_test = df_ref_wh.drop(df_ref_wh_train.index)\n",
    "\n",
    "#get data class oil, foam, dirt, water from exp 1,2,3,4\n",
    "#get training data\n",
    "df_water_all=df_add[df_add['class'] == 'water']\n",
    "df_water_1=df_water_all[df_water_all['exp'].isin([1])] \n",
    "df_water_2=df_water_all[df_water_all['exp'].isin([2])] \n",
    "df_water_3=df_water_all[df_water_all['exp'].isin([3])] \n",
    "df_water_4=df_water_all[df_water_all['exp'].isin([4])] \n",
    "df_water_5=df_water_all[df_water_all['exp'].isin([5])] \n",
    "\n",
    "\n",
    "df_oil_all=df_add[df_add['class'] == 'oil']\n",
    "df_oil_1=df_oil_all[df_oil_all['exp'].isin([1])] \n",
    "df_oil_2=df_oil_all[df_oil_all['exp'].isin([2])] \n",
    "df_oil_3=df_oil_all[df_oil_all['exp'].isin([3])] \n",
    "df_oil_4=df_oil_all[df_oil_all['exp'].isin([4])] \n",
    "df_oil_5=df_oil_all[df_oil_all['exp'].isin([5])] \n",
    "\n",
    "df_foam_all=df_add[df_add['class'] == 'foam']\n",
    "df_foam_1=df_foam_all[df_foam_all['exp'].isin([1])] \n",
    "df_foam_2=df_foam_all[df_foam_all['exp'].isin([2])] \n",
    "df_foam_3=df_foam_all[df_foam_all['exp'].isin([3])] \n",
    "df_foam_4=df_foam_all[df_foam_all['exp'].isin([4])] \n",
    "df_foam_5=df_foam_all[df_foam_all['exp'].isin([5])] \n",
    "\n",
    "df_dirt_all=df_add[df_add['class'] == 'dirt']\n",
    "df_dirt_1=df_dirt_all[df_dirt_all['exp'].isin([1])] \n",
    "df_dirt_2=df_dirt_all[df_dirt_all['exp'].isin([2])] \n",
    "df_dirt_3=df_dirt_all[df_dirt_all['exp'].isin([3])] \n",
    "df_dirt_4=df_dirt_all[df_dirt_all['exp'].isin([4])] \n",
    "df_dirt_5=df_dirt_all[df_dirt_all['exp'].isin([5])] \n",
    "\n",
    "\n",
    "#skenario A 1234\n",
    "#list dataframe you want to append\n",
    "frame_A_train = [df_ref_train, df_ref_wh_train, df_water_1, df_water_2, df_water_3, df_water_4, df_oil_1, df_oil_2, df_oil_3, df_oil_4, df_foam_1, df_foam_2, df_foam_3, df_foam_4,df_dirt_1, df_dirt_2, df_dirt_3, df_dirt_4]\n",
    "\n",
    "df_A_train = pd.DataFrame()\n",
    "\n",
    "for df in frame_A_train:\n",
    "    df_A_train = df_A_train.append(df)\n",
    "\n",
    "frame_A_test = [df_ref_test, df_ref_wh_test, df_water_5, df_oil_5, df_foam_5, df_dirt_5]\n",
    "\n",
    "df_A_test = pd.DataFrame()\n",
    "\n",
    "for df in frame_A_test:\n",
    "    df_A_test = df_A_test.append(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization_per_feature(df_train, df_test):\n",
    "    # constant parameter to normalized coordinate and attribute\n",
    "    # That value is taken from Dataset exp 1,2,3,4 over all type of contamiant\n",
    "    # do normalization for testing dataset using value from training dataset\n",
    "    #  \n",
    "    x_min = 8.136452 # minimum coordinate point in x-axis\n",
    "    y_min = -0.313579 # minimum coordinate point in y-axis\n",
    "    z_min = -0.260452 # minimum coordinate point in z-axis\n",
    "\n",
    "    x_max = 8.992307 # maximum coordinate point in x-axis\n",
    "    y_max = 0.499952 # maximum coordinate point in y-axis\n",
    "    z_max = 0.145646 # maximum coordinate point in z-axis\n",
    "\n",
    "    x_range = x_max-x_min\n",
    "    y_range = y_max-y_min\n",
    "    z_range = z_max-z_min\n",
    "\n",
    "    a_min = 0.0 # minimun value of ambient\n",
    "    r_min = 59.0 # minimun value of reflectivity\n",
    "\n",
    "    a_max = 472.0 # maximum value of ambient\n",
    "    r_max = 19528.0 # maximum value of reflectivity\n",
    "\n",
    "    a_range = a_max-a_min # minimun value of ambient\n",
    "    r_range = r_max-r_min # range value of reflectivity\n",
    "\n",
    "\n",
    "    df_add_norm=df_train.copy()\n",
    "    df_add_norm['count_x']=(df_add_norm['count_x']-df_train.min()[0])/(df_train.max()[0]-df_train.min()[0])\n",
    "    df_add_norm['mean_x']=(df_add_norm['mean_x']-x_min)/x_range\n",
    "    df_add_norm['std_x']=(df_add_norm['std_x']-x_min)/x_range\n",
    "    df_add_norm['min_x']=(df_add_norm['min_x']-x_min)/x_range\n",
    "    df_add_norm['per_25_x']=(df_add_norm['per_25_x']-x_min)/x_range\n",
    "    df_add_norm['per_50_x']=(df_add_norm['per_50_x']-x_min)/x_range\n",
    "    df_add_norm['per_75_x']=(df_add_norm['per_75_x']-x_min)/x_range\n",
    "    df_add_norm['max_x']=(df_add_norm['max_x']-x_min)/x_range\n",
    "    \n",
    "    df_add_norm['mean_y']=(df_add_norm['mean_y']-y_min)/y_range\n",
    "    df_add_norm['std_y']=(df_add_norm['std_y']-y_min)/y_range\n",
    "    df_add_norm['min_y']=(df_add_norm['min_y']-y_min)/y_range\n",
    "    df_add_norm['per_25_y']=(df_add_norm['per_25_y']-y_min)/y_range\n",
    "    df_add_norm['per_50_y']=(df_add_norm['per_50_y']-y_min)/y_range\n",
    "    df_add_norm['per_75_y']=(df_add_norm['per_75_y']-y_min)/y_range\n",
    "    df_add_norm['max_y']=(df_add_norm['max_y']-y_min)/y_range\n",
    "\n",
    "    df_add_norm['mean_z']=(df_add_norm['mean_z']-z_min)/z_range\n",
    "    df_add_norm['std_z']=(df_add_norm['std_z']-z_min)/z_range\n",
    "    df_add_norm['min_z']=(df_add_norm['min_z']-z_min)/z_range\n",
    "    df_add_norm['per_25_z']=(df_add_norm['per_25_z']-z_min)/z_range\n",
    "    df_add_norm['per_50_z']=(df_add_norm['per_50_z']-z_min)/z_range\n",
    "    df_add_norm['per_75_z']=(df_add_norm['per_75_z']-z_min)/z_range\n",
    "    df_add_norm['max_z']=(df_add_norm['max_z']-z_min)/z_range\n",
    "\n",
    "    df_add_norm['mean_ref']=(df_add_norm['mean_ref']-r_min)/r_range\n",
    "    df_add_norm['std_ref']=(df_add_norm['std_ref']-r_min)/r_range\n",
    "    df_add_norm['min_ref']=(df_add_norm['min_ref']-r_min)/r_range\n",
    "    df_add_norm['per_25_ref']=(df_add_norm['per_25_ref']-r_min)/r_range\n",
    "    df_add_norm['per_50_ref']=(df_add_norm['per_50_ref']-r_min)/r_range\n",
    "    df_add_norm['per_75_ref']=(df_add_norm['per_75_ref']-r_min)/r_range\n",
    "    df_add_norm['max_ref']=(df_add_norm['max_ref']-r_min)/r_range\n",
    "\n",
    "    df_add_norm['mean_amb']=(df_add_norm['mean_amb']-a_min)/a_range\n",
    "    df_add_norm['std_amb']=(df_add_norm['std_amb']-a_min)/a_range\n",
    "    df_add_norm['min_amb']=(df_add_norm['min_amb']-a_min)/a_range\n",
    "    df_add_norm['per_25_amb']=(df_add_norm['per_25_amb']-a_min)/a_range\n",
    "    df_add_norm['per_50_amb']=(df_add_norm['per_50_amb']-a_min)/a_range\n",
    "    df_add_norm['per_75_amb']=(df_add_norm['per_75_amb']-a_min)/a_range\n",
    "    df_add_norm['max_amb']=(df_add_norm['max_amb']-a_min)/a_range\n",
    "\n",
    "    # Data test\n",
    "    df_add_norm_test = df_test.copy()\n",
    "    df_add_norm_test['count_x']=(df_add_norm_test['count_x']-df_train.min()[0])/(df_train.max()[0]-df_train.min()[0])\n",
    "    df_add_norm_test['mean_x']=(df_add_norm_test['mean_x']-x_min)/x_range\n",
    "    df_add_norm_test['std_x']=(df_add_norm_test['std_x']-x_min)/x_range\n",
    "    df_add_norm_test['min_x']=(df_add_norm_test['min_x']-x_min)/x_range\n",
    "    df_add_norm_test['per_25_x']=(df_add_norm_test['per_25_x']-x_min)/x_range\n",
    "    df_add_norm_test['per_50_x']=(df_add_norm_test['per_50_x']-x_min)/x_range\n",
    "    df_add_norm_test['per_75_x']=(df_add_norm_test['per_75_x']-x_min)/x_range\n",
    "    df_add_norm_test['max_x']=(df_add_norm_test['max_x']-x_min)/x_range\n",
    "    \n",
    "    df_add_norm_test['mean_y']=(df_add_norm_test['mean_y']-y_min)/y_range\n",
    "    df_add_norm_test['std_y']=(df_add_norm_test['std_y']-y_min)/y_range\n",
    "    df_add_norm_test['min_y']=(df_add_norm_test['min_y']-y_min)/y_range\n",
    "    df_add_norm_test['per_25_y']=(df_add_norm_test['per_25_y']-y_min)/y_range\n",
    "    df_add_norm_test['per_50_y']=(df_add_norm_test['per_50_y']-y_min)/y_range\n",
    "    df_add_norm_test['per_75_y']=(df_add_norm_test['per_75_y']-y_min)/y_range\n",
    "    df_add_norm_test['max_y']=(df_add_norm_test['max_y']-y_min)/y_range\n",
    "\n",
    "    df_add_norm_test['mean_z']=(df_add_norm_test['mean_z']-z_min)/z_range\n",
    "    df_add_norm_test['std_z']=(df_add_norm_test['std_z']-z_min)/z_range\n",
    "    df_add_norm_test['min_z']=(df_add_norm_test['min_z']-z_min)/z_range\n",
    "    df_add_norm_test['per_25_z']=(df_add_norm_test['per_25_z']-z_min)/z_range\n",
    "    df_add_norm_test['per_50_z']=(df_add_norm_test['per_50_z']-z_min)/z_range\n",
    "    df_add_norm_test['per_75_z']=(df_add_norm_test['per_75_z']-z_min)/z_range\n",
    "    df_add_norm_test['max_z']=(df_add_norm_test['max_z']-z_min)/z_range\n",
    "\n",
    "    df_add_norm_test['mean_ref']=(df_add_norm_test['mean_ref']-r_min)/r_range\n",
    "    df_add_norm_test['std_ref']=(df_add_norm_test['std_ref']-r_min)/r_range\n",
    "    df_add_norm_test['min_ref']=(df_add_norm_test['min_ref']-r_min)/r_range\n",
    "    df_add_norm_test['per_25_ref']=(df_add_norm_test['per_25_ref']-r_min)/r_range\n",
    "    df_add_norm_test['per_50_ref']=(df_add_norm_test['per_50_ref']-r_min)/r_range\n",
    "    df_add_norm_test['per_75_ref']=(df_add_norm_test['per_75_ref']-r_min)/r_range\n",
    "    df_add_norm_test['max_ref']=(df_add_norm_test['max_ref']-r_min)/r_range\n",
    "\n",
    "    df_add_norm_test['mean_amb']=(df_add_norm_test['mean_amb']-a_min)/a_range\n",
    "    df_add_norm_test['std_amb']=(df_add_norm_test['std_amb']-a_min)/a_range\n",
    "    df_add_norm_test['min_amb']=(df_add_norm_test['min_amb']-a_min)/a_range\n",
    "    df_add_norm_test['per_25_amb']=(df_add_norm_test['per_25_amb']-a_min)/a_range\n",
    "    df_add_norm_test['per_50_amb']=(df_add_norm_test['per_50_amb']-a_min)/a_range\n",
    "    df_add_norm_test['per_75_amb']=(df_add_norm_test['per_75_amb']-a_min)/a_range\n",
    "    df_add_norm_test['max_amb']=(df_add_norm_test['max_amb']-a_min)/a_range\n",
    "\n",
    "\n",
    "    return df_add_norm, df_add_norm_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_A_train_clean =  df_A_train.drop('exp', axis=1)\n",
    "df_A_test_clean =  df_A_test.drop('exp', axis=1)\n",
    "\n",
    "df_A_train_clean = df_A_train_clean.sample(frac = 1,random_state=1)\n",
    "df_A_test_clean = df_A_test_clean.sample(frac = 1,random_state=1)\n",
    "\n",
    "df_A_train_X = df_A_train_clean.drop('class', axis=1)\n",
    "df_A_train_Y = df_A_train_clean['class']\n",
    "\n",
    "df_A_test_X = df_A_test_clean.drop('class', axis=1)\n",
    "df_A_test_Y = df_A_test_clean['class']\n",
    "\n",
    "\n",
    "# Do normalization based over all contaminant per feature\n",
    "# Data train and data test\n",
    "df_train_norm, df_test_norm = normalization_per_feature(df_A_train_X, df_A_test_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "\"logit\": LogisticRegression(solver=\"lbfgs\", multi_class=\"auto\"),\n",
    "\"mlp\": MLPClassifier(),\n",
    "\"svm_rbf\": SVC(kernel = \"rbf\", random_state = 0),\n",
    "\"naive_bayes\": GaussianNB(),\n",
    "\"knn\": KNeighborsClassifier(n_neighbors=5),\n",
    "\"decision_tree\": DecisionTreeClassifier(),\n",
    "\"random_forest\": RandomForestClassifier(n_estimators=10)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_train_Y = df_A_train_Y.map({'clean':0, 'cover':1, 'dirt':2,'foam':3, 'oil':4, 'water':5})\n",
    "encoded_test_Y = df_A_test_Y.map({'clean':0, 'cover':1, 'dirt':2,'foam':3, 'oil':4, 'water':5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready Training\n"
     ]
    }
   ],
   "source": [
    "class_name = ['clean', 'cover',  'dirt', 'foam', 'oil', 'water']\n",
    "labels= ['clean', 'cover','dirt', 'foam', 'oil', 'water']\n",
    "\n",
    "print(\"Ready Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Performance Measurement using model - fold: logit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gjati/lidar-cover-contm/py3.8LidCov/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Performance Measurement using model - fold: mlp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gjati/lidar-cover-contm/py3.8LidCov/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Performance Measurement using model - fold: svm_rbf\n",
      "[INFO] Performance Measurement using model - fold: naive_bayes\n",
      "[INFO] Performance Measurement using model - fold: knn\n",
      "[INFO] Performance Measurement using model - fold: decision_tree\n",
      "[INFO] Performance Measurement using model - fold: random_forest\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for model_name, model in models.items():\n",
    "        # print(key)\n",
    "        # print(model)\n",
    "\n",
    "        print(\"[INFO] Performance Measurement using model - fold:\", model_name)\n",
    "        \n",
    "        \n",
    "        t = ti.time()\n",
    "        model.fit(df_train_norm, encoded_train_Y)\n",
    "        elapsed_training = ti.time() - t\n",
    "\n",
    "        t = ti.time()    \n",
    "        Y_pred_class = model.predict(df_test_norm)\n",
    "        elapsed_inference = ti.time() - t\n",
    "        elapsed_instance = elapsed_inference/(len(df_test_norm))\n",
    "\n",
    "\n",
    "\n",
    "        Y_val_class = encoded_test_Y.values\n",
    "        # print(Y_val_class)\n",
    "        # print(Y_pred_class)\n",
    "\n",
    "        #save model\n",
    "        filename = \"1D_class_norm_per_exp_train_test_model/model_\"+model_name+\".pickle\"\n",
    "        pickle.dump(model, open(filename, \"wb\"))\n",
    "\n",
    "\n",
    "        # print(classification_report(Y_val_class,Y_pred_class, target_names=class_name))\n",
    "        with open(\"1D_class_norm_per_exp_train_test_result.txt\", \"a\") as f:\n",
    "            print(\"model_name: \",model_name, file=f)\n",
    "            print(classification_report(Y_val_class,Y_pred_class, target_names=class_name), file=f)\n",
    "            \n",
    "            print(\"elapsed_training: \", elapsed_training, file=f)\n",
    "            # print(\"Inference time per Fold:\", elapsed_inference, file=f)\n",
    "\n",
    "            # print(\"Inference instance:\", len(df_test_X), file=f)\n",
    "            # print(\"Inference time per Instance:\", elapsed_instance, file=f)\n",
    "            print(\"elapsed_instance: \",elapsed_instance, file=f)\n",
    "\n",
    "            if model_name==\"random_forest\" or model_name==\"decision_tree\" :\n",
    "                # get importance\n",
    "                importance = model.feature_importances_\n",
    "                # summarize feature importance\n",
    "                for iii,vvv in enumerate(importance):\n",
    "                    print('Feature: %0d, Score: %.5f' % (iii,vvv), file=f)\n",
    "            \n",
    "            print(\"==============\", file=f)\n",
    "\n",
    "        \n",
    "        # compute the confusion matrix\n",
    "        confusion_mtx = confusion_matrix(Y_val_class, Y_pred_class) \n",
    "\n",
    "        # plot the confusion matrix\n",
    "        plot_confusion_matrix(model_name,confusion_mtx,classes = class_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.8LidCov",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
