{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yey\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "import math\n",
    "import time as ti\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "from sklearn.metrics import classification_report # performance measurement , untuk menghasilkan confusion matrix\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  dev = \"cuda:3\"\n",
    "  print(\"yey\")\n",
    "device = torch.device(dev)\n",
    "\n",
    "# Define relevant variables for the ML task\n",
    "batch_size = 32\n",
    "num_classes = 6\n",
    "learning_rate = 0.03 #0.01\n",
    "num_epochs = 100 #150 #100 best\n",
    "\n",
    "\n",
    "# Define a function to plot confusion matrix\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig('3D_class_norm_per_exp_train_test_11x11_conf/all-exp-'+'3D_class_norm_per_exp_train_test-11x11-'+'.png')\n",
    "    \n",
    "    plt.clf()\n",
    "\n",
    "#load all dataset\n",
    "\n",
    "df = pd.read_json('../get_3D_voxel_Norm_per_exp_train_test/lidar_norm_train_test_3d_cube_RA_6class_11x11x11.json')\n",
    "\n",
    "\n",
    "df.drop(df[df['class'] == 'dew'].index, inplace = True)\n",
    "df.drop(df[df['class'] == 'dirt_05mm'].index, inplace = True)\n",
    "df.drop(df[df['class'] == 'dirt_10mm'].index, inplace = True)\n",
    "df.drop(df[df['class'] == 'dirt_15mm'].index, inplace = True)\n",
    "\n",
    "df_add = df\n",
    "\n",
    "#get data class=ref without cover\n",
    "df_no_cover =df_add[df_add['class'] == 'no_cover']\n",
    "df_ref_train = df_no_cover.sample(frac=0.80,random_state=1)\n",
    "df_ref_test = df_no_cover.drop(df_ref_train.index)\n",
    "\n",
    "\n",
    "#get data class=ref with cover\n",
    "df_cover =df_add[df_add['class'] == 'cover']\n",
    "df_ref_wh_train = df_cover.sample(frac=0.80,random_state=1)\n",
    "df_ref_wh_test = df_cover.drop(df_ref_wh_train.index)\n",
    "\n",
    "#get data class oil, foam, dirt, water from exp 1,2,3,4\n",
    "#get training data\n",
    "df_water_all=df_add[df_add['class'] == 'water']\n",
    "df_water_1=df_water_all[df_water_all['exp'].isin([1])] \n",
    "df_water_2=df_water_all[df_water_all['exp'].isin([2])] \n",
    "df_water_3=df_water_all[df_water_all['exp'].isin([3])] \n",
    "df_water_4=df_water_all[df_water_all['exp'].isin([4])] \n",
    "df_water_5=df_water_all[df_water_all['exp'].isin([5])] \n",
    "\n",
    "\n",
    "df_oil_all=df_add[df_add['class'] == 'oil']\n",
    "df_oil_1=df_oil_all[df_oil_all['exp'].isin([1])] \n",
    "df_oil_2=df_oil_all[df_oil_all['exp'].isin([2])] \n",
    "df_oil_3=df_oil_all[df_oil_all['exp'].isin([3])] \n",
    "df_oil_4=df_oil_all[df_oil_all['exp'].isin([4])] \n",
    "df_oil_5=df_oil_all[df_oil_all['exp'].isin([5])] \n",
    "\n",
    "df_foam_all=df_add[df_add['class'] == 'foam']\n",
    "df_foam_1=df_foam_all[df_foam_all['exp'].isin([1])] \n",
    "df_foam_2=df_foam_all[df_foam_all['exp'].isin([2])] \n",
    "df_foam_3=df_foam_all[df_foam_all['exp'].isin([3])] \n",
    "df_foam_4=df_foam_all[df_foam_all['exp'].isin([4])] \n",
    "df_foam_5=df_foam_all[df_foam_all['exp'].isin([5])] \n",
    "\n",
    "df_dirt_all=df_add[df_add['class'] == 'dirt']\n",
    "df_dirt_1=df_dirt_all[df_dirt_all['exp'].isin([1])] \n",
    "df_dirt_2=df_dirt_all[df_dirt_all['exp'].isin([2])] \n",
    "df_dirt_3=df_dirt_all[df_dirt_all['exp'].isin([3])] \n",
    "df_dirt_4=df_dirt_all[df_dirt_all['exp'].isin([4])] \n",
    "df_dirt_5=df_dirt_all[df_dirt_all['exp'].isin([5])] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatChannels(x):\n",
    "    size = x.size()\n",
    "    b = x.view(size[0],size[1],size[2]*size[3]*size[4])\n",
    "    return x.view(size[0],size[1],size[2]*size[3]*size[4])\n",
    "\n",
    "def globalAvgPool2D(x):        \n",
    "    return flatChannels(x).mean(dim=-1)\n",
    "\n",
    "def globalMaxPool2D(x):\n",
    "    return flatChannels(x).max(dim=-1)\n",
    "    \n",
    "class Unit(nn.Module):\n",
    "    def __init__(self,in_channels,out_channels):\n",
    "        super(Unit,self).__init__()\n",
    "        self.conv = nn.Conv3d(in_channels=in_channels,kernel_size=(3,3,3),out_channels=out_channels,stride=1,padding=1) # BEST for 11x11x11\n",
    "  \n",
    "        # self.bn = nn.BatchNorm2d(num_features=out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self,input):\n",
    "        output = self.conv(input)\n",
    "        # output = self.bn(output)\n",
    "        output = self.relu(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self,num_classes):\n",
    "        super(Net,self).__init__()\n",
    "\n",
    "        #Create 14 layers of the unit with max pooling in between\n",
    "        self.unit0 = Unit(in_channels=2,out_channels=16)\n",
    "        self.unit15 = Unit(in_channels=16,out_channels=32)\n",
    "        self.unit16 = Unit(in_channels=32,out_channels=64)\n",
    "\n",
    "        # self.avgpool = nn.AvgPool3d(kernel_size=2)\n",
    "        \n",
    "        #Add all the units into the Sequential layer in exact order\n",
    "        self.net = nn.Sequential(self.unit0, self.unit15, self.unit16) #BEST F1-99\n",
    "        self.fc = nn.Linear(in_features=64,out_features=num_classes)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.net(input)\n",
    "        output = globalAvgPool2D(output)\n",
    "         \n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(num_classes).cuda(device=device)\n",
    "# summary(net, (2, 10, 10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(num_epochs): #Training your network\n",
    "    losses = []\n",
    "    for epoch in range(num_epochs): # loop over the dataset multiple times\n",
    "        # running_loss = 0.0\n",
    "\n",
    "        train_acc = 0.0\n",
    "        train_loss = 0.0\n",
    "    \n",
    "        for i, data in enumerate(list_train_tensor, 0):\n",
    "            #get the input; data is a list of [input, labels]\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device=device), labels.to(device=device)\n",
    "            \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward pass \n",
    "            outputs = net(inputs)\n",
    "    \n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # backward and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, prediction = torch.max(outputs.data, 1)\n",
    "\n",
    "            train_acc += torch.sum(prediction == labels.data)\n",
    "\n",
    "        #Compute the average acc and loss over all 50000 training images\n",
    "        train_acc = train_acc / df_train_X_tensor.size()[0]\n",
    "        train_loss = train_loss / df_train_X_tensor.size()[0]\n",
    "\n",
    "         # Print the metrics\n",
    "        print(\"Epoch {}, Train Accuracy: {} , Train Loss: {}\".format(epoch, train_acc, train_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2864677/598870456.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_A_train = df_A_train.append(df)\n",
      "/tmp/ipykernel_2864677/598870456.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_A_train = df_A_train.append(df)\n",
      "/tmp/ipykernel_2864677/598870456.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_A_train = df_A_train.append(df)\n",
      "/tmp/ipykernel_2864677/598870456.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_A_train = df_A_train.append(df)\n",
      "/tmp/ipykernel_2864677/598870456.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_A_train = df_A_train.append(df)\n",
      "/tmp/ipykernel_2864677/598870456.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_A_train = df_A_train.append(df)\n",
      "/tmp/ipykernel_2864677/598870456.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_A_train = df_A_train.append(df)\n",
      "/tmp/ipykernel_2864677/598870456.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_A_train = df_A_train.append(df)\n",
      "/tmp/ipykernel_2864677/598870456.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_A_train = df_A_train.append(df)\n",
      "/tmp/ipykernel_2864677/598870456.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_A_train = df_A_train.append(df)\n",
      "/tmp/ipykernel_2864677/598870456.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_A_train = df_A_train.append(df)\n",
      "/tmp/ipykernel_2864677/598870456.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_A_train = df_A_train.append(df)\n",
      "/tmp/ipykernel_2864677/598870456.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_A_train = df_A_train.append(df)\n",
      "/tmp/ipykernel_2864677/598870456.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_A_train = df_A_train.append(df)\n",
      "/tmp/ipykernel_2864677/598870456.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_A_train = df_A_train.append(df)\n",
      "/tmp/ipykernel_2864677/598870456.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_A_train = df_A_train.append(df)\n",
      "/tmp/ipykernel_2864677/598870456.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_A_train = df_A_train.append(df)\n",
      "/tmp/ipykernel_2864677/598870456.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_A_train = df_A_train.append(df)\n",
      "/tmp/ipykernel_2864677/598870456.py:14: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_A_test = df_A_test.append(df)\n",
      "/tmp/ipykernel_2864677/598870456.py:14: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_A_test = df_A_test.append(df)\n",
      "/tmp/ipykernel_2864677/598870456.py:14: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_A_test = df_A_test.append(df)\n",
      "/tmp/ipykernel_2864677/598870456.py:14: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_A_test = df_A_test.append(df)\n",
      "/tmp/ipykernel_2864677/598870456.py:14: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_A_test = df_A_test.append(df)\n",
      "/tmp/ipykernel_2864677/598870456.py:14: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_A_test = df_A_test.append(df)\n"
     ]
    }
   ],
   "source": [
    "#splt data train and data test\n",
    "frame_A_train = [df_ref_train, df_ref_wh_train, df_water_1, df_water_2, df_water_3, df_water_4, df_oil_1, df_oil_2, df_oil_3, df_oil_4, df_foam_1, df_foam_2, df_foam_3, df_foam_4,df_dirt_1, df_dirt_2, df_dirt_3, df_dirt_4]\n",
    "\n",
    "df_A_train = pd.DataFrame()\n",
    "\n",
    "for df in frame_A_train:\n",
    "    df_A_train = df_A_train.append(df)\n",
    "\n",
    "frame_A_test = [df_ref_test, df_ref_wh_test, df_water_5, df_oil_5, df_foam_5, df_dirt_5]\n",
    "\n",
    "df_A_test = pd.DataFrame()\n",
    "\n",
    "for df in frame_A_test:\n",
    "    df_A_test = df_A_test.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_A_train_clean =  df_A_train.drop('exp', axis=1)\n",
    "df_A_test_clean =  df_A_test.drop('exp', axis=1)\n",
    "\n",
    "df_A_train_clean = df_A_train_clean.sample(frac = 1,random_state=1)\n",
    "df_A_test_clean = df_A_test_clean.sample(frac = 1,random_state=1)\n",
    "\n",
    "df_A_train_X = df_A_train_clean.drop('class', axis=1)\n",
    "df_A_train_Y = df_A_train_clean['class']\n",
    "\n",
    "df_A_test_X = df_A_test_clean.drop('class', axis=1)\n",
    "df_A_test_Y = df_A_test_clean['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_X = df_A_train_X\n",
    "df_train_Y = df_A_train_Y\n",
    "\n",
    "df_test_X = df_A_test_X\n",
    "df_test_Y = df_A_test_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: \n",
      "Epoch 0, Train Accuracy: 0.23823606967926025 , Train Loss: 0.05137452270818609\n",
      "Epoch 1, Train Accuracy: 0.3241462707519531 , Train Loss: 0.04450063246749812\n",
      "Epoch 2, Train Accuracy: 0.4143586754798889 , Train Loss: 0.038163549567012574\n",
      "Epoch 3, Train Accuracy: 0.48453882336616516 , Train Loss: 0.03519718173082941\n",
      "Epoch 4, Train Accuracy: 0.5529711842536926 , Train Loss: 0.03118289840833639\n",
      "Epoch 5, Train Accuracy: 0.5592901110649109 , Train Loss: 0.030042650214636573\n",
      "Epoch 6, Train Accuracy: 0.7303038239479065 , Train Loss: 0.01884342733250731\n",
      "Epoch 7, Train Accuracy: 0.848480761051178 , Train Loss: 0.012755243836406995\n",
      "Epoch 8, Train Accuracy: 0.8874697089195251 , Train Loss: 0.00938226204264321\n",
      "Epoch 9, Train Accuracy: 0.8987630605697632 , Train Loss: 0.00772944080329576\n",
      "Epoch 10, Train Accuracy: 0.9033342003822327 , Train Loss: 0.006867206957202248\n",
      "Epoch 11, Train Accuracy: 0.9116697907447815 , Train Loss: 0.006318904024491982\n",
      "Epoch 12, Train Accuracy: 0.9185264706611633 , Train Loss: 0.005920224982326584\n",
      "Epoch 13, Train Accuracy: 0.9228286743164062 , Train Loss: 0.005518152721829234\n",
      "Epoch 14, Train Accuracy: 0.9278031587600708 , Train Loss: 0.005232329471835641\n",
      "Epoch 15, Train Accuracy: 0.9326431155204773 , Train Loss: 0.004988620089380555\n",
      "Epoch 16, Train Accuracy: 0.9364075660705566 , Train Loss: 0.00480549205753188\n",
      "Epoch 17, Train Accuracy: 0.9417853951454163 , Train Loss: 0.004374147095745287\n",
      "Epoch 18, Train Accuracy: 0.9450120329856873 , Train Loss: 0.0040213605323889045\n",
      "Epoch 19, Train Accuracy: 0.9181231260299683 , Train Loss: 0.007735247280134144\n",
      "Epoch 20, Train Accuracy: 0.9459531903266907 , Train Loss: 0.003940734585008876\n",
      "Epoch 21, Train Accuracy: 0.9515998363494873 , Train Loss: 0.0036664915936727312\n",
      "Epoch 22, Train Accuracy: 0.9552298784255981 , Train Loss: 0.003524968848541443\n",
      "Epoch 23, Train Accuracy: 0.9600698947906494 , Train Loss: 0.003276221898581475\n",
      "Epoch 24, Train Accuracy: 0.962489902973175 , Train Loss: 0.0031357678603786605\n",
      "Epoch 25, Train Accuracy: 0.9630276560783386 , Train Loss: 0.0031151926939048395\n",
      "Epoch 26, Train Accuracy: 0.9650443196296692 , Train Loss: 0.0029364625305918018\n",
      "Epoch 27, Train Accuracy: 0.967195451259613 , Train Loss: 0.0027989381207436177\n",
      "Epoch 28, Train Accuracy: 0.9690776467323303 , Train Loss: 0.00272694894125919\n",
      "Epoch 29, Train Accuracy: 0.9705565571784973 , Train Loss: 0.0025580028830718177\n",
      "Epoch 30, Train Accuracy: 0.9717665910720825 , Train Loss: 0.0024418689980870052\n",
      "Epoch 31, Train Accuracy: 0.9735143184661865 , Train Loss: 0.002302322987170675\n",
      "Epoch 32, Train Accuracy: 0.9732454419136047 , Train Loss: 0.002261688610497896\n",
      "Epoch 33, Train Accuracy: 0.9749932289123535 , Train Loss: 0.002173356042859831\n",
      "Epoch 34, Train Accuracy: 0.9757999181747437 , Train Loss: 0.0021106596057846416\n",
      "Epoch 35, Train Accuracy: 0.9763376712799072 , Train Loss: 0.0020654965638458272\n",
      "Epoch 36, Train Accuracy: 0.978085458278656 , Train Loss: 0.001993662478024369\n",
      "Epoch 37, Train Accuracy: 0.9774132370948792 , Train Loss: 0.001991413251742767\n",
      "Epoch 38, Train Accuracy: 0.979564368724823 , Train Loss: 0.0018805010715295676\n",
      "Epoch 39, Train Accuracy: 0.971497654914856 , Train Loss: 0.002581310311858734\n",
      "Epoch 40, Train Accuracy: 0.9794299006462097 , Train Loss: 0.0018273582966243445\n",
      "Epoch 41, Train Accuracy: 0.9796987771987915 , Train Loss: 0.0017970058586574643\n",
      "Epoch 42, Train Accuracy: 0.9811776876449585 , Train Loss: 0.0017319368864325733\n",
      "Epoch 43, Train Accuracy: 0.9809088110923767 , Train Loss: 0.0017255094125163202\n",
      "Epoch 44, Train Accuracy: 0.979564368724823 , Train Loss: 0.0017731597185478968\n",
      "Epoch 45, Train Accuracy: 0.979564368724823 , Train Loss: 0.0018177853823527013\n",
      "Epoch 46, Train Accuracy: 0.9817155003547668 , Train Loss: 0.0016712734487188696\n",
      "Epoch 47, Train Accuracy: 0.9826565980911255 , Train Loss: 0.001584168208925027\n",
      "Epoch 48, Train Accuracy: 0.9837321639060974 , Train Loss: 0.0015634471429379182\n",
      "Epoch 49, Train Accuracy: 0.9837321639060974 , Train Loss: 0.0015503881063059866\n",
      "Epoch 50, Train Accuracy: 0.9845387935638428 , Train Loss: 0.0015061220467852143\n",
      "Epoch 51, Train Accuracy: 0.9840010404586792 , Train Loss: 0.0015421841642381106\n",
      "Epoch 52, Train Accuracy: 0.9840010404586792 , Train Loss: 0.0015047226765970106\n",
      "Epoch 53, Train Accuracy: 0.984269917011261 , Train Loss: 0.00145132168433793\n",
      "Epoch 54, Train Accuracy: 0.9849421381950378 , Train Loss: 0.001443519876398643\n",
      "Epoch 55, Train Accuracy: 0.982791006565094 , Train Loss: 0.001652307564831677\n",
      "Epoch 56, Train Accuracy: 0.9829254746437073 , Train Loss: 0.0015440330700316517\n",
      "Epoch 57, Train Accuracy: 0.9626243114471436 , Train Loss: 0.004002355707185276\n",
      "Epoch 58, Train Accuracy: 0.979564368724823 , Train Loss: 0.001852030817172123\n",
      "Epoch 59, Train Accuracy: 0.9817155003547668 , Train Loss: 0.001688105383881376\n",
      "Epoch 60, Train Accuracy: 0.9830599427223206 , Train Loss: 0.0016338124252678228\n",
      "Epoch 61, Train Accuracy: 0.9831943511962891 , Train Loss: 0.001667217337722963\n",
      "Epoch 62, Train Accuracy: 0.9844043850898743 , Train Loss: 0.0015738550550110275\n",
      "Epoch 63, Train Accuracy: 0.984673261642456 , Train Loss: 0.0015030745865293068\n",
      "Epoch 64, Train Accuracy: 0.9853454828262329 , Train Loss: 0.0014124705249942743\n",
      "Epoch 65, Train Accuracy: 0.9866899251937866 , Train Loss: 0.001354632155972744\n",
      "Epoch 66, Train Accuracy: 0.9860177040100098 , Train Loss: 0.0013500519247292876\n",
      "Epoch 67, Train Accuracy: 0.9865554571151733 , Train Loss: 0.0013312481036633864\n",
      "Epoch 68, Train Accuracy: 0.9868243932723999 , Train Loss: 0.0013451750755723757\n",
      "Epoch 69, Train Accuracy: 0.9865554571151733 , Train Loss: 0.0013462764437443874\n",
      "Epoch 70, Train Accuracy: 0.9869588017463684 , Train Loss: 0.0012967014303785264\n",
      "Epoch 71, Train Accuracy: 0.9862865805625916 , Train Loss: 0.0012938158571470701\n",
      "Epoch 72, Train Accuracy: 0.9870932698249817 , Train Loss: 0.0012992536603405763\n",
      "Epoch 73, Train Accuracy: 0.9873621463775635 , Train Loss: 0.001291422496035041\n",
      "Epoch 74, Train Accuracy: 0.9869588017463684 , Train Loss: 0.0012800064706606688\n",
      "Epoch 75, Train Accuracy: 0.9870932698249817 , Train Loss: 0.0013205341457574106\n",
      "Epoch 76, Train Accuracy: 0.9872276782989502 , Train Loss: 0.0012911067220257681\n",
      "Epoch 77, Train Accuracy: 0.9868243932723999 , Train Loss: 0.001333070128450108\n",
      "Epoch 78, Train Accuracy: 0.9874966144561768 , Train Loss: 0.001262586409001681\n",
      "Epoch 79, Train Accuracy: 0.9880343675613403 , Train Loss: 0.0012962467981360675\n",
      "Epoch 80, Train Accuracy: 0.9873621463775635 , Train Loss: 0.0012816997618960635\n",
      "Epoch 81, Train Accuracy: 0.9877654910087585 , Train Loss: 0.001273360499615963\n",
      "Epoch 82, Train Accuracy: 0.9884377121925354 , Train Loss: 0.0012366462524070771\n",
      "Epoch 83, Train Accuracy: 0.9881688356399536 , Train Loss: 0.0012263376037253387\n",
      "Epoch 84, Train Accuracy: 0.9881688356399536 , Train Loss: 0.0012334465649533385\n",
      "Epoch 85, Train Accuracy: 0.9881688356399536 , Train Loss: 0.001214238618333749\n",
      "Epoch 86, Train Accuracy: 0.9884377121925354 , Train Loss: 0.0012144142402532334\n",
      "Epoch 87, Train Accuracy: 0.9888410568237305 , Train Loss: 0.0012083918091660862\n",
      "Epoch 88, Train Accuracy: 0.9884377121925354 , Train Loss: 0.001194701306756038\n",
      "Epoch 89, Train Accuracy: 0.9880343675613403 , Train Loss: 0.0012336942450849966\n",
      "Epoch 90, Train Accuracy: 0.9885721802711487 , Train Loss: 0.0012210966659624417\n",
      "Epoch 91, Train Accuracy: 0.9884377121925354 , Train Loss: 0.0012023837399780053\n",
      "Epoch 92, Train Accuracy: 0.9885721802711487 , Train Loss: 0.0012087270632009572\n",
      "Epoch 93, Train Accuracy: 0.9887065887451172 , Train Loss: 0.001207358767897976\n",
      "Epoch 94, Train Accuracy: 0.988975465297699 , Train Loss: 0.001200178104872122\n",
      "Epoch 95, Train Accuracy: 0.9891099333763123 , Train Loss: 0.0012025185895596139\n",
      "Epoch 96, Train Accuracy: 0.989378809928894 , Train Loss: 0.0011566131309821875\n",
      "Epoch 97, Train Accuracy: 0.989378809928894 , Train Loss: 0.0011895428093218624\n",
      "Epoch 98, Train Accuracy: 0.9892444014549255 , Train Loss: 0.001198880910594755\n",
      "Epoch 99, Train Accuracy: 0.989378809928894 , Train Loss: 0.0011452736838741524\n",
      "Accuracy of the network on the 1792 test images: 97 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_train_Y = df_train_Y.map({'no_cover':0, 'cover':1, 'dirt':2, 'foam':3, 'oil':4, 'water':5})\n",
    "encoded_test_Y = df_test_Y.map({'no_cover':0, 'cover':1, 'dirt':2, 'foam':3, 'oil':4, 'water':5})\n",
    "\n",
    "Y_train_dummy=encoded_train_Y\n",
    "Y_test_dummy=encoded_test_Y\n",
    "\n",
    "\n",
    "#convert data train to tensor\n",
    "list_df_train_X = []\n",
    "for item in range(len(df_train_X['data'])):\n",
    "    list_df_train_X.append(df_train_X['data'].iloc[item])\n",
    "\n",
    "# df_A_train_X_tensor = torch.FloatTensor(list_df_A_train_X,device=device)\n",
    "df_train_X_tensor = torch.FloatTensor(list_df_train_X)\n",
    "\n",
    "# print(df_train_X_tensor.shape)\n",
    "\n",
    "\n",
    "#convert data test to tensor\n",
    "list_df_test_X = []\n",
    "for item in range(len(df_test_X['data'])):\n",
    "    list_df_test_X.append(df_test_X['data'].iloc[item])\n",
    "\n",
    "# df_A_test_X_tensor = torch.FloatTensor(list_df_A_test_X,device=device)\n",
    "df_test_X_tensor = torch.FloatTensor(list_df_test_X)\n",
    "\n",
    "\n",
    "# convert label to tensor\n",
    "encoded_train_Y_tensor = torch.from_numpy(Y_train_dummy.to_numpy())\n",
    "encoded_test_Y_tensor = torch.from_numpy(Y_test_dummy.to_numpy())\n",
    "\n",
    "# # get batch for each FOLD TRAINING    \n",
    "num_batches = math.ceil(df_train_X_tensor.size()[0]/batch_size)\n",
    "train_X_raw = [df_train_X_tensor[batch_size*y:batch_size*(y+1),:,:] for y in range(num_batches)]\n",
    "# print(train_X_raw[0].size())\n",
    "\n",
    "num_batches = math.ceil(encoded_train_Y_tensor.size()[0]/batch_size)\n",
    "train_Y_raw = [encoded_train_Y_tensor[batch_size*y:batch_size*(y+1)] for y in range(num_batches)]\n",
    "# print(train_Y_raw[0].size())\n",
    "\n",
    "List_train_row = len(train_X_raw)\n",
    "List_train_columns = 2\n",
    "list_train_tensor = [[ 0 for x in range(List_train_columns)] for i in range (List_train_row)]\n",
    "\n",
    "for i in range(List_train_row):\n",
    "    for j in range(List_train_columns):\n",
    "        list_train_tensor[i][0]=train_X_raw[i]\n",
    "        list_train_tensor[i][1]=train_Y_raw[i]\n",
    "\n",
    "\n",
    "# # get batch for each FOLD TESTING\n",
    "num_batches = math.ceil(df_test_X_tensor.size()[0]/batch_size)\n",
    "test_X_raw = [df_test_X_tensor[batch_size*y:batch_size*(y+1),:,:] for y in range(num_batches)]\n",
    "# print(test_X_raw[0].size())\n",
    "\n",
    "num_batches = math.ceil(encoded_test_Y_tensor.size()[0]/batch_size)\n",
    "test_Y_raw = [encoded_test_Y_tensor[batch_size*y:batch_size*(y+1)] for y in range(num_batches)]\n",
    "# print(test_Y_raw[0].size())\n",
    "\n",
    "List_test_row = len(test_X_raw)\n",
    "List_test_columns = 2\n",
    "list_test_tensor = [[ 0 for x in range(List_test_columns)] for i in range (List_test_row)]\n",
    "\n",
    "for i in range(List_test_row):\n",
    "    for j in range(List_test_columns):\n",
    "        list_test_tensor[i][0]=test_X_raw[i]\n",
    "        list_test_tensor[i][1]=test_Y_raw[i]\n",
    "\n",
    "print(\"Training: \")\n",
    "\n",
    "#initialize the network\n",
    "net = Net(num_classes).cuda(device=device)\n",
    "# print(\"model\")\n",
    "# print(net) # print model summary\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "criterion = criterion.cuda(device=device)\n",
    "\n",
    "t = ti.time()\n",
    "\n",
    "train_net(num_epochs)\n",
    "elapsed_training = ti.time() - t\n",
    "training_time = elapsed_training\n",
    "\n",
    "#save model\n",
    "torch.save(net, \"model_3D_class_norm_per_exp_train_test_11x11x11.pt\")\n",
    "class_name = ['clean', 'cover','dirt', 'foam', 'oil', 'water']\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "list_test_labels = []\n",
    "list_test_predicted = []\n",
    "\n",
    "timings_inference = []\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(list_test_tensor, 0):\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        t = ti.time()\n",
    "        outputs = net(images)\n",
    "        elapsed_training = ti.time() - t\n",
    "        timings_inference.append(elapsed_training)\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        labels_cpu = labels.tolist()\n",
    "        predicted_cpu = predicted.tolist() \n",
    "        for jj in range(len(labels_cpu)):\n",
    "\n",
    "            list_test_labels.append(labels_cpu[jj])\n",
    "            list_test_predicted.append(predicted_cpu[jj])\n",
    "\n",
    "\n",
    "sum_syn = np.sum(timings_inference) # inference time per epoch\n",
    "mean_syn_per_instance = np.sum(timings_inference)/len(list_test_predicted) # inference time per epoch\n",
    "\n",
    "correct_CPU = 0\n",
    "total_CPU = 0\n",
    "list_test_labels_CPU = []\n",
    "list_test_predicted_CPU = []\n",
    "\n",
    "timings_inference_CPU = []\n",
    "\n",
    "net_CPU = net.cpu()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(list_test_tensor, 0):\n",
    "        images, labels = data\n",
    "        # images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        t = ti.time()\n",
    "        outputs = net_CPU(images)\n",
    "        elapsed_training = ti.time() - t\n",
    "\n",
    "        \n",
    "        timings_inference_CPU.append(elapsed_training)\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_CPU += labels.size(0)\n",
    "        correct_CPU += (predicted == labels).sum().item()\n",
    "\n",
    "        labels_cpu = labels.tolist()\n",
    "        predicted_cpu = predicted.tolist() \n",
    "        for jj in range(len(labels_cpu)):\n",
    "\n",
    "            list_test_labels_CPU.append(labels_cpu[jj])\n",
    "            list_test_predicted_CPU.append(predicted_cpu[jj])\n",
    "\n",
    "sum_syn_CPU = np.sum(timings_inference_CPU) # inference time per epoch\n",
    "mean_syn_per_instance_CPU = np.sum(timings_inference_CPU)/len(list_test_predicted_CPU) # inference time per epoch\n",
    "\n",
    "\n",
    "with open(\"output_3D_class_norm_per_exp_train_test_11x11x11.txt\", \"a\") as f:\n",
    "# with open(\"output_3D_class_norm_per_exp_train_test_6x6x6.txt\", \"a\") as f:\n",
    "# with open(\"output_3D_class_norm_per_exp_train_test_8x8x8.txt\", \"a\") as f:\n",
    "\n",
    "    print(\"CNN 3D 3 CH: \", file=f)\n",
    "    print(classification_report(list_test_labels,list_test_predicted,target_names=class_name), file=f)\n",
    "    print(\"Training time per Fold:\", training_time, file=f)\n",
    "    print(\"Inference time per Fold:\", sum_syn, file=f)\n",
    "\n",
    "    print(\"Inference instance:\", len(list_test_predicted), file=f)\n",
    "    print(\"Inference time per instance:\", mean_syn_per_instance, file=f)\n",
    "\n",
    "    print(\"=====CPU=========\", file=f)\n",
    "\n",
    "    print(classification_report(list_test_labels_CPU,list_test_predicted_CPU,target_names=class_name), file=f)\n",
    "    print(\"Inference time per Fold CPU:\", sum_syn_CPU, file=f)\n",
    "    print(\"Inference instance CPU:\", len(list_test_predicted_CPU), file=f)\n",
    "    print(\"Inference time per instance CPU:\", mean_syn_per_instance_CPU, file=f)\n",
    "    print(\"==============\", file=f)\n",
    "    # compute the confusion matrix     \n",
    "#     #    \n",
    "print('Accuracy of the network on the %d test images: %d %%' % (num_batches*batch_size,\n",
    "    100 * correct / total))\n",
    "confusion_mtx = confusion_matrix(list_test_labels, list_test_predicted) \n",
    "\n",
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(confusion_mtx,class_name) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.8LidCov",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
